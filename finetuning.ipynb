{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NoorBolbol/ArabicEmpathicDataset/blob/main/finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "utwQWH4rVGxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version\n",
        "!sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.8\n",
        "!sudo update-alternatives --config python3\n",
        "!python3 --version\n",
        "!sudo apt install python3-pip\n"
      ],
      "metadata": {
        "id": "7io-mZZ5rHXU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b2a9357-9fc5-4f71-b602-786596be3e61"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.11\n",
            "update-alternatives: --install needs <link> <name> <path> <priority>\n",
            "\n",
            "Use 'update-alternatives --help' for program usage information.\n",
            "There are 2 choices for the alternative python3 (providing /usr/bin/python3).\n",
            "\n",
            "  Selection    Path                 Priority   Status\n",
            "------------------------------------------------------------\n",
            "* 0            /usr/bin/python3.10   2         auto mode\n",
            "  1            /usr/bin/python3.10   2         manual mode\n",
            "  2            /usr/bin/python3.8    1         manual mode\n",
            "\n",
            "Press <enter> to keep the current choice[*], or type selection number: 2\n",
            "update-alternatives: using /usr/bin/python3.8 to provide /usr/bin/python3 (python3) in manual mode\n",
            "Python 3.8.10\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  python-pip-whl python3-setuptools python3-wheel\n",
            "Suggested packages:\n",
            "  python-setuptools-doc\n",
            "The following NEW packages will be installed:\n",
            "  python-pip-whl python3-pip python3-setuptools python3-wheel\n",
            "0 upgraded, 4 newly installed, 0 to remove and 24 not upgraded.\n",
            "Need to get 2,389 kB of archives.\n",
            "After this operation, 4,933 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 python-pip-whl all 20.0.2-5ubuntu1.8 [1,805 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 python3-setuptools all 45.2.0-1ubuntu0.1 [330 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 python3-wheel all 0.34.2-1ubuntu0.1 [23.9 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 python3-pip all 20.0.2-5ubuntu1.8 [231 kB]\n",
            "Fetched 2,389 kB in 1s (2,658 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 4.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package python-pip-whl.\n",
            "(Reading database ... 122531 files and directories currently installed.)\n",
            "Preparing to unpack .../python-pip-whl_20.0.2-5ubuntu1.8_all.deb ...\n",
            "Unpacking python-pip-whl (20.0.2-5ubuntu1.8) ...\n",
            "Selecting previously unselected package python3-setuptools.\n",
            "Preparing to unpack .../python3-setuptools_45.2.0-1ubuntu0.1_all.deb ...\n",
            "Unpacking python3-setuptools (45.2.0-1ubuntu0.1) ...\n",
            "Selecting previously unselected package python3-wheel.\n",
            "Preparing to unpack .../python3-wheel_0.34.2-1ubuntu0.1_all.deb ...\n",
            "Unpacking python3-wheel (0.34.2-1ubuntu0.1) ...\n",
            "Selecting previously unselected package python3-pip.\n",
            "Preparing to unpack .../python3-pip_20.0.2-5ubuntu1.8_all.deb ...\n",
            "Unpacking python3-pip (20.0.2-5ubuntu1.8) ...\n",
            "Setting up python3-setuptools (45.2.0-1ubuntu0.1) ...\n",
            "Setting up python3-wheel (0.34.2-1ubuntu0.1) ...\n",
            "Setting up python-pip-whl (20.0.2-5ubuntu1.8) ...\n",
            "Setting up python3-pip (20.0.2-5ubuntu1.8) ...\n",
            "Processing triggers for man-db (2.9.1-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install python3.8-venv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAvwAYx-W2DI",
        "outputId": "0a140160-6b39-40b9-8d08-90efb14e9668"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  python3.8-venv\n",
            "0 upgraded, 1 newly installed, 0 to remove and 24 not upgraded.\n",
            "Need to get 5,452 B of archives.\n",
            "After this operation, 27.6 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 python3.8-venv amd64 3.8.10-0ubuntu1~20.04.7 [5,452 B]\n",
            "Fetched 5,452 B in 0s (179 kB/s)\n",
            "Selecting previously unselected package python3.8-venv.\n",
            "(Reading database ... 122895 files and directories currently installed.)\n",
            "Preparing to unpack .../python3.8-venv_3.8.10-0ubuntu1~20.04.7_amd64.deb ...\n",
            "Unpacking python3.8-venv (3.8.10-0ubuntu1~20.04.7) ...\n",
            "Setting up python3.8-venv (3.8.10-0ubuntu1~20.04.7) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3.8 -m venv myenv # Create a virtual environment\n",
        "!source myenv/bin/activate  # Activate the virtual environment\n",
        "!pip install datasets  # Install the 'datasets' package\n",
        "import sys\n",
        "print(sys.version)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "u9IFio1HTELZ",
        "outputId": "43870c6a-2ab6-4182-e7af-87d12550435f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.12.0-py3-none-any.whl (474 kB)\n",
            "\u001b[K     |████████████████████████████████| 474 kB 15.6 MB/s \n",
            "\u001b[?25hCollecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py38-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 71.9 MB/s \n",
            "\u001b[?25hCollecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Collecting dill<0.3.7,>=0.3.0\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[K     |████████████████████████████████| 110 kB 78.8 MB/s \n",
            "\u001b[?25hCollecting numpy>=1.17\n",
            "  Downloading numpy-1.24.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 17.3 MB 70.0 MB/s \n",
            "\u001b[?25hCollecting tqdm>=4.62.1\n",
            "  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 6.4 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[K     |████████████████████████████████| 224 kB 77.0 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (701 kB)\n",
            "\u001b[K     |████████████████████████████████| 701 kB 69.9 MB/s \n",
            "\u001b[?25hCollecting fsspec[http]>=2021.11.1\n",
            "  Downloading fsspec-2023.5.0-py3-none-any.whl (160 kB)\n",
            "\u001b[K     |████████████████████████████████| 160 kB 76.8 MB/s \n",
            "\u001b[?25hCollecting aiohttp\n",
            "  Downloading aiohttp-3.8.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 63.6 MB/s \n",
            "\u001b[?25hCollecting pandas\n",
            "  Downloading pandas-2.0.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.3 MB 68.5 MB/s \n",
            "\u001b[?25hCollecting packaging\n",
            "  Downloading packaging-23.1-py3-none-any.whl (48 kB)\n",
            "\u001b[K     |████████████████████████████████| 48 kB 1.2 MB/s \n",
            "\u001b[?25hCollecting requests>=2.19.0\n",
            "  Downloading requests-2.30.0-py3-none-any.whl (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 1.3 MB/s \n",
            "\u001b[?25hCollecting pyarrow>=8.0.0\n",
            "  Downloading pyarrow-12.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (39.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 39.0 MB 1.3 MB/s \n",
            "\u001b[?25hCollecting xxhash\n",
            "  Downloading xxhash-3.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
            "\u001b[K     |████████████████████████████████| 213 kB 75.4 MB/s \n",
            "\u001b[?25hCollecting urllib3>=1.25.10\n",
            "  Downloading urllib3-2.0.2-py3-none-any.whl (123 kB)\n",
            "\u001b[K     |████████████████████████████████| 123 kB 78.2 MB/s \n",
            "\u001b[?25hCollecting filelock\n",
            "  Downloading filelock-3.12.0-py3-none-any.whl (10 kB)\n",
            "Collecting typing-extensions>=3.7.4.3\n",
            "  Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting attrs>=17.3.0\n",
            "  Downloading attrs-23.1.0-py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 8.7 MB/s \n",
            "\u001b[?25hCollecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (161 kB)\n",
            "\u001b[K     |████████████████████████████████| 161 kB 80.7 MB/s \n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (121 kB)\n",
            "\u001b[K     |████████████████████████████████| 121 kB 55.5 MB/s \n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting charset-normalizer<4.0,>=2.0\n",
            "  Downloading charset_normalizer-3.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (195 kB)\n",
            "\u001b[K     |████████████████████████████████| 195 kB 77.4 MB/s \n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.9.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (266 kB)\n",
            "\u001b[K     |████████████████████████████████| 266 kB 74.2 MB/s \n",
            "\u001b[?25hCollecting python-dateutil>=2.8.2\n",
            "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
            "\u001b[K     |████████████████████████████████| 247 kB 80.2 MB/s \n",
            "\u001b[?25hCollecting pytz>=2020.1\n",
            "  Downloading pytz-2023.3-py2.py3-none-any.whl (502 kB)\n",
            "\u001b[K     |████████████████████████████████| 502 kB 73.5 MB/s \n",
            "\u001b[?25hCollecting tzdata>=2022.1\n",
            "  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
            "\u001b[K     |████████████████████████████████| 341 kB 81.8 MB/s \n",
            "\u001b[?25hCollecting idna<4,>=2.5\n",
            "  Downloading idna-3.4-py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 123 kB/s \n",
            "\u001b[?25hCollecting certifi>=2017.4.17\n",
            "  Downloading certifi-2023.5.7-py3-none-any.whl (156 kB)\n",
            "\u001b[K     |████████████████████████████████| 156 kB 79.7 MB/s \n",
            "\u001b[?25hCollecting six>=1.5\n",
            "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
            "Installing collected packages: dill, multiprocess, urllib3, charset-normalizer, idna, certifi, requests, responses, numpy, tqdm, filelock, packaging, typing-extensions, pyyaml, async-timeout, attrs, frozenlist, multidict, aiosignal, yarl, aiohttp, fsspec, huggingface-hub, six, python-dateutil, pytz, tzdata, pandas, pyarrow, xxhash, datasets\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 attrs-23.1.0 certifi-2023.5.7 charset-normalizer-3.1.0 datasets-2.12.0 dill-0.3.6 filelock-3.12.0 frozenlist-1.3.3 fsspec-2023.5.0 huggingface-hub-0.14.1 idna-3.4 multidict-6.0.4 multiprocess-0.70.14 numpy-1.24.3 packaging-23.1 pandas-2.0.1 pyarrow-12.0.0 python-dateutil-2.8.2 pytz-2023.3 pyyaml-6.0 requests-2.30.0 responses-0.18.0 six-1.16.0 tqdm-4.65.0 typing-extensions-4.5.0 tzdata-2023.3 urllib3-2.0.2 xxhash-3.2.0 yarl-1.9.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "dateutil",
                  "six"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.10.11 (main, Apr  5 2023, 14:15:10) [GCC 9.4.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gfa3sg6AI7T",
        "outputId": "7e1067c5-9681-479d-aa82-c8ecea98ab4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers==4.2\n",
            "  Downloading transformers-4.2.0-py3-none-any.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 17.0 MB/s \n",
            "\u001b[?25hCollecting regex!=2019.12.17\n",
            "  Downloading regex-2023.5.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (771 kB)\n",
            "\u001b[K     |████████████████████████████████| 771 kB 70.5 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 69.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from transformers==4.2) (23.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers==4.2) (3.12.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers==4.2) (2.30.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers==4.2) (4.65.0)\n",
            "Collecting tokenizers==0.9.4\n",
            "  Downloading tokenizers-0.9.4-cp38-cp38-manylinux2010_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 59.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from transformers==4.2) (1.24.3)\n",
            "Collecting click\n",
            "  Downloading click-8.1.3-py3-none-any.whl (96 kB)\n",
            "\u001b[K     |████████████████████████████████| 96 kB 6.3 MB/s \n",
            "\u001b[?25hCollecting joblib\n",
            "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
            "\u001b[K     |████████████████████████████████| 297 kB 77.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers==4.2) (1.16.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.2) (2.0.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.2) (2023.5.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.2) (3.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.2) (3.1.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895255 sha256=336a038a3b2e4d7760be8c94be6055a8910749aedf27a4ae040b1c2541d51eb9\n",
            "  Stored in directory: /root/.cache/pip/wheels/82/ab/9b/c15899bf659ba74f623ac776e861cf2eb8608c1825ddec66a4\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: regex, click, joblib, sacremoses, tokenizers, transformers\n",
            "Successfully installed click-8.1.3 joblib-1.2.0 regex-2023.5.5 sacremoses-0.0.53 tokenizers-0.9.4 transformers-4.2.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting arabert\n",
            "  Downloading arabert-1.0.1-py3-none-any.whl (179 kB)\n",
            "\u001b[K     |████████████████████████████████| 179 kB 12.9 MB/s \n",
            "\u001b[?25hCollecting emoji==1.4.2\n",
            "  Downloading emoji-1.4.2.tar.gz (184 kB)\n",
            "\u001b[K     |████████████████████████████████| 184 kB 110.2 MB/s \n",
            "\u001b[?25hCollecting farasapy\n",
            "  Downloading farasapy-0.0.14-py3-none-any.whl (11 kB)\n",
            "Collecting PyArabic\n",
            "  Downloading PyArabic-0.6.15-py3-none-any.whl (126 kB)\n",
            "\u001b[K     |████████████████████████████████| 126 kB 73.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from farasapy->arabert) (2.30.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from farasapy->arabert) (4.65.0)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.8/dist-packages (from PyArabic->arabert) (1.16.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->farasapy->arabert) (2023.5.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->farasapy->arabert) (2.0.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->farasapy->arabert) (3.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->farasapy->arabert) (3.1.0)\n",
            "Building wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.4.2-py3-none-any.whl size=186450 sha256=cba1952fcb17aeb39ced456547a13eab28722227f6c0631215357a597d023f71\n",
            "  Stored in directory: /root/.cache/pip/wheels/71/4d/3c/cada364d4ea0026deee7208dee1e61bcebd20aa2ae5dc154ba\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji, farasapy, PyArabic, arabert\n",
            "Successfully installed PyArabic-0.6.15 arabert-1.0.1 emoji-1.4.2 farasapy-0.0.14\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers==4.2\n",
        "# !pip install databases\n",
        "!pip install arabert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "R5KOj4nCBwxJ",
        "outputId": "9dc8c693-6a1a-4f7c-ab54-9362cbf85d21"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-5b37d3d49a21>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#Fetch dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'wget https://raw.githubusercontent.com/NoorBolbol/ArabicEmpathicDataset/main/finetuned-empathic.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'datasets'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "from datasets import load_dataset \n",
        "import transformers\n",
        "#Fetch dataset\n",
        "!wget https://raw.githubusercontent.com/NoorBolbol/ArabicEmpathicDataset/main/finetuned-empathic.csv\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "all_data = load_dataset(\"/content/drive/MyDrive/Thesis/Arabic-Empathetic-Chatbot-master/model/ArabicEmpatheticDialogues.py\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CzNhp6IqCadM"
      },
      "outputs": [],
      "source": [
        "print(all_data['train'][:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T7MR6wFyFKYb"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizerFast\n",
        "encoder_max_length=75\n",
        "decoder_max_length=75\n",
        "model_name = \"tareknaous/bert2bert-empathetic-response-msa\"#\"aubmindlab/bert-base-arabert\"\n",
        "\n",
        "tokenizer = BertTokenizerFast.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "41cjUh2_DlfM"
      },
      "outputs": [],
      "source": [
        "train_data = all_data['train'].train_test_split(test_size=0.2,seed=42)['train']\n",
        "val_data = all_data['train'].train_test_split(test_size=0.2,seed=42)['test']\n",
        "dev_data = val_data.train_test_split(test_size=0.5,seed=42)['train']\n",
        "test_data = val_data.train_test_split(test_size=0.5,seed=42)['test']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nn1ZX_YNEJ42"
      },
      "outputs": [],
      "source": [
        "print(\"Length of train data\",len(train_data))\n",
        "print(\"Length of dev data\",len(dev_data))\n",
        "print(\"Length of test data\",len(test_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fPQaJEDqELqo"
      },
      "outputs": [],
      "source": [
        "def process_data_to_model_inputs(batch):                                                               \n",
        "    # Tokenizer will automatically set [BOS] <text> [EOS]                                               \n",
        "    inputs = tokenizer(batch[\"context\"], padding=\"max_length\", truncation=True, max_length=encoder_max_length)\n",
        "    outputs = tokenizer(batch[\"response\"], padding=\"max_length\", truncation=True, max_length=decoder_max_length)\n",
        "                                                                       \n",
        "    batch[\"input_ids\"] = inputs.input_ids                                                               \n",
        "    batch[\"attention_mask\"] = inputs.attention_mask                                                     \n",
        "    batch[\"decoder_input_ids\"] = outputs.input_ids                                                      \n",
        "    batch[\"labels\"] = outputs.input_ids.copy()                                                          \n",
        "    # mask loss for padding                                                                             \n",
        "    batch[\"labels\"] = [                                                                                 \n",
        "        [-100 if token == tokenizer.pad_token_id else token for token in labels] for labels in batch[\"labels\"]\n",
        "    ]                     \n",
        "    batch[\"decoder_attention_mask\"] = outputs.attention_mask                                                                              \n",
        "                                                                                                         \n",
        "    return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZpCsNoaQEtme"
      },
      "outputs": [],
      "source": [
        "batch_size=16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r1VKxzAtEkfA"
      },
      "outputs": [],
      "source": [
        "train_data = train_data.map(\n",
        "    process_data_to_model_inputs, \n",
        "    batched=True, \n",
        "    batch_size=batch_size, \n",
        "    remove_columns=[\"context\", \"response\"],\n",
        ")\n",
        "train_data.set_format(\n",
        "    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"decoder_attention_mask\", \"labels\"],\n",
        ")\n",
        "\n",
        "dev_data = dev_data.map(\n",
        "    process_data_to_model_inputs, \n",
        "    batched=True, \n",
        "    batch_size=batch_size, \n",
        "    remove_columns=[\"context\", \"response\"],\n",
        ")\n",
        "dev_data.set_format(\n",
        "    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"decoder_attention_mask\", \"labels\"],\n",
        ")\n",
        "\n",
        "test_data = test_data.map(\n",
        "    process_data_to_model_inputs, \n",
        "    batched=True, \n",
        "    batch_size=batch_size, \n",
        "    remove_columns=[\"context\", \"response\"],\n",
        ")\n",
        "test_data.set_format(\n",
        "    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"decoder_attention_mask\", \"labels\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfeipkNpGL5o"
      },
      "outputs": [],
      "source": [
        "from transformers import EncoderDecoderModel\n",
        "\n",
        "arabert2arabert = EncoderDecoderModel.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xVvr4Baeu6Z3"
      },
      "outputs": [],
      "source": [
        "#set special tokens\n",
        "arabert2arabert.config.decoder_start_token_id = tokenizer.cls_token_id                                             \n",
        "arabert2arabert.config.eos_token_id = tokenizer.sep_token_id\n",
        "arabert2arabert.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "#sensible parameters for beam search\n",
        "#set decoding params                               \n",
        "arabert2arabert.config.max_length = 64\n",
        "arabert2arabert.config.early_stopping = True\n",
        "\n",
        "arabert2arabert.config.num_beams = 1\n",
        "arabert2arabert.config.vocab_size = arabert2arabert.config.encoder.vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OM3Rcr4Ky6T9"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Optional\n",
        "\n",
        "@dataclass\n",
        "class Seq2SeqTrainingArguments(TrainingArguments):\n",
        "    label_smoothing: Optional[float] = field(\n",
        "        default=0.0, metadata={\"help\": \"The label smoothing epsilon to apply (if not zero).\"}\n",
        "    )\n",
        "    sortish_sampler: bool = field(default=False, metadata={\"help\": \"Whether to SortishSamler or not.\"})\n",
        "    predict_with_generate: bool = field(\n",
        "        default=False, metadata={\"help\": \"Whether to use generate to calculate generative metrics (ROUGE, BLEU).\"}\n",
        "    )\n",
        "    adafactor: bool = field(default=False, metadata={\"help\": \"whether to use adafactor\"})\n",
        "    encoder_layerdrop: Optional[float] = field(\n",
        "        default=None, metadata={\"help\": \"Encoder layer dropout probability. Goes into model.config.\"}\n",
        "    )\n",
        "    decoder_layerdrop: Optional[float] = field(\n",
        "        default=None, metadata={\"help\": \"Decoder layer dropout probability. Goes into model.config.\"}\n",
        "    )\n",
        "    dropout: Optional[float] = field(default=None, metadata={\"help\": \"Dropout probability. Goes into model.config.\"})\n",
        "    attention_dropout: Optional[float] = field(\n",
        "        default=None, metadata={\"help\": \"Attention dropout probability. Goes into model.config.\"}\n",
        "    )\n",
        "    lr_scheduler: Optional[str] = field(\n",
        "        default=\"linear\", metadata={\"help\": f\"Which lr scheduler to use.\"}\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iAKKc6itGoN-"
      },
      "outputs": [],
      "source": [
        "! pip install sacrebleu==1.4.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kcRPh8jUu9P-"
      },
      "outputs": [],
      "source": [
        "from sacrebleu import corpus_bleu\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
        "\n",
        "def compute_metrics(pred):\n",
        "  labels_ids = pred.label_ids\n",
        "  #pred_ids = torch.argmax(pred.predictions,dim=2)\n",
        "  pred_ids = pred.predictions  \n",
        "\n",
        "  # all unnecessary tokens are removed\n",
        "  pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "  labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n",
        "  label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
        "\n",
        "  return {\"bleu\": round(corpus_bleu(pred_str , [label_str]).score, 4)}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install --upgrade accelerate"
      ],
      "metadata": {
        "id": "Kk3RLn_xDhOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers[torch]"
      ],
      "metadata": {
        "id": "J91cwAfOD8VO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y transformers accelerate\n",
        "!pip install transformers accelerate"
      ],
      "metadata": {
        "id": "9KC92m64pQHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5xhytujGu_IK"
      },
      "outputs": [],
      "source": [
        "#Set training arguments \n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./model\",\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size // 2,\n",
        "    gradient_accumulation_steps=2,\n",
        "    predict_with_generate=True,\n",
        "    do_eval=True,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    do_train=True,\n",
        "    logging_steps=500,\n",
        "    save_steps=32965 // (batch_size * 2),\n",
        "    warmup_steps=100,\n",
        "    eval_steps=10,\n",
        "    num_train_epochs=5,\n",
        "    overwrite_output_dir=True,\n",
        "    save_total_limit=0,\n",
        "    fp16=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install --upgrade transformers==4.2 --no-dependencies"
      ],
      "metadata": {
        "id": "O6WB_Dwrp7lw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eu7R1jpRn4Ff"
      },
      "outputs": [],
      "source": [
        "from transformers import Seq2SeqTrainer\n",
        "# instantiate trainer\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=arabert2arabert,\n",
        "    args=training_args,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        "    train_dataset=train_data,\n",
        "    eval_dataset=dev_data,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z53vKJIkHSkI"
      },
      "outputs": [],
      "source": [
        "#Train\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NVPNuvYWHX-Y"
      },
      "outputs": [],
      "source": [
        "#Evaluate\n",
        "eval_output = trainer.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZ5w5vDsxIPV"
      },
      "outputs": [],
      "source": [
        "print(eval_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "13VhXnAcHgcA"
      },
      "outputs": [],
      "source": [
        "#Compute perplexity\n",
        "import math\n",
        "perplexity = math.exp(eval_output[\"eval_loss\"])\n",
        "print('\\nEvaluate Perplexity: {:10,.2f}'.format(perplexity))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yv4pAbXTIePL"
      },
      "outputs": [],
      "source": [
        "#Save tokenizer and model\n",
        "trainer.save_model(\"./arabert2arabert\")\n",
        "tokenizer.save_pretrained(\"./arabert2arabert\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggBQe7ZKJDBp"
      },
      "source": [
        "**Gradio Demo** \\\\\n",
        "This allows you to create a sharable web application of the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-Gr46vHh7Hk"
      },
      "outputs": [],
      "source": [
        "!pip install gradio\n",
        "import gradio as gr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ux9WG4Q-JEns"
      },
      "outputs": [],
      "source": [
        "from transformers import EncoderDecoderModel, AutoTokenizer\n",
        "from datasets import load_dataset \n",
        "from arabert.preprocess import ArabertPreprocessor\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from transformers import default_data_collator\n",
        "from torch.utils.data.sampler import SequentialSampler\n",
        "import torch\n",
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t9ss8ggxJrzd"
      },
      "outputs": [],
      "source": [
        "model_name=\"bert-base-arabert\"\n",
        "arabert_prep = ArabertPreprocessor(model_name=model_name, keep_emojis=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TuHRn5CSJGa3"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"noorbolbol/master-thesis-model\")\n",
        "model = EncoderDecoderModel.from_pretrained(\"noorbolbol/master-thesis-model\")\n",
        "\n",
        "model.to(\"cuda\")\n",
        "model.eval()\n",
        "print(\"done\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97v_PxuHI700"
      },
      "outputs": [],
      "source": [
        "def generate_response(text, minimum_length, k):\n",
        "  text_clean = arabert_prep.preprocess(text)\n",
        "  inputs = tokenizer.encode_plus(text_clean,return_tensors='pt')\n",
        "  outputs = model.generate(input_ids = inputs.input_ids.to(\"cuda\"),\n",
        "                   attention_mask = inputs.attention_mask.to(\"cuda\"),\n",
        "                   num_beams=1,\n",
        "                   do_sample = True,\n",
        "                   min_length=minimum_length,\n",
        "                   top_k = k,\n",
        "                   temperature = 1,\n",
        "                   length_penalty =2)\n",
        "  preds = tokenizer.batch_decode(outputs) \n",
        "  response = str(preds)\n",
        "  response = response.replace(\"\\'\", '')\n",
        "  response = response.replace(\"[[CLS]\", '')\n",
        "  response = response.replace(\"[SEP]]\", '')\n",
        "  response = str(arabert_prep.desegment(response))\n",
        "  return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cQZ1XJRWmDa_"
      },
      "outputs": [],
      "source": [
        "text = \" صديقاتي محبطات\"\n",
        "print(generate_response(text,10,20))\n",
        "# أوه لا ، ما الذي يجعل كتشعر بالخوف من العالم الخارجي ؟ أو هل لديك شيء محدد محدد لكمن حولك ؟ \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! git lfs install\n",
        "! git clone https://huggingface.co/noorbolbol/master-thesis-model"
      ],
      "metadata": {
        "id": "4VgTFKFkE9z2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "! git init /master-thersis-model"
      ],
      "metadata": {
        "id": "xeTlNGhcGmTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ./master-thesis-model"
      ],
      "metadata": {
        "id": "k-4boZCcJvkn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! git config --global user.email \"noorbolbol8@gmail.com\"\n",
        "! git config --global user.name \"Noor Bolbol\""
      ],
      "metadata": {
        "id": "RxbNxcUUJ4F1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install huggingface_hub"
      ],
      "metadata": {
        "id": "pOiex-ehO2EI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! git add --all\n"
      ],
      "metadata": {
        "id": "Jqm_kFixGs52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! git commit -m \"first commit\" "
      ],
      "metadata": {
        "id": "GWoYrz0FSCr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! git status "
      ],
      "metadata": {
        "id": "vyewfuwwIRAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! huggingface-cli login \n"
      ],
      "metadata": {
        "id": "wijngRiwXXoF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! git config --global credential.helper store"
      ],
      "metadata": {
        "id": "NtTYK1ZoXgfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! git pull"
      ],
      "metadata": {
        "id": "07wOHPX_X3FU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! git push\n"
      ],
      "metadata": {
        "id": "TANok7-mM2nu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! git mv ../arabert2arabert/.* ./\n",
        "\n"
      ],
      "metadata": {
        "id": "VdcQfBSTtVVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D9I5imXq690y"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPVxF010A5IX5oK8dJswALH",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}